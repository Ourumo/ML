1) 기계 학습에서 학습이란 무엇인지를 정리하시오

- 학습이란 훈련데이터로부터 가중치매개변수의 최적값을 자동으로 획득 하는것

 처음 들어오는 데이터에서 다음 노드로 넘어갈때 모두 같은값이라면 계속 같은 값이 나오기 때문에 각기 다르게 곱해야한다.
 이때 다르게 곱해야 한다는것이 가중치이다. 가중치는 각 신호의 영향력을 조절하는 매개변수이다. 학습 알고리즘이 이러한 가중치를 조정하여 모델의 성능을 최적화 시킨다.

- 손실함수는 지도학습시 알고리즘이 예측한 값과 실제 정답의 차이를 비교하기 위한 함수이다.
즉, 학습 중에 알고리즘이 얼마나 잘못 예측했는지 그 정도를 확인하기 위한 함수이다. 모델의 최적화를 위햐 손실함수를 최소화 시켜야한다.

- 손실함수종류

MSE(평균제곱오차)
 손실함수의 값을 전체 데이터셋에 대해서 실제값과 예측값의 차이를 제곱한 후 평균을 낸 값이다. 분산과 비슷한 개념이다. MSE의 값이 크다는 것은 평균사이에 차이가 크다는 뜻이다.

binary_crossentropy(이항 교차 엔트로피)
 y값이 2개인 이진분류기를 훈련할 때 자주 사용되는 손실함수이다. 실제값을 1이라고 했을 때 예측값도 1이면 수식이 0에 수렵하게 되는 손실함수이다.

- 학습이란
 모델은 초기에 설정된 가중치와 훈련 데이터를 사용하여 학습한다. 모델이 예측한 값의 손실 함수를 계산하고 손실 함수가 최소화하는 방향으로 가중치를 조절해 가며 모델을 학습시킨다. 손실함수를 최소화 하여 모델이 주어진 작업을 최적으로 수행하는것이 목표이다.

2) 확률적 경사 하강법 소스코드 분석

- 확률적 경사 하강법이란 매 스텝에서 한 개의 샘플을 랜덤하게 선택하고 그 하나의 샘플에 대한 그레디언트를 계산하는 방법이다.
 배치 경사 하강법에 비해 적은 데이터로 학습이 가능하다.
 적절한 학습률을 선택해야한다. 학습률이 너무 크면 발산할 수 있고 너무 작다면 수렵이 더디게 될수 있다.

n_epochs = 50: 전체 데이터셋을 몇 번 반복해서 사용하여 학습할 것인지를 지정(50번의 에폭(epoch) 동안 학습)

t0, t1 = 5,50 : 학습 스케줄(learning schedule)에 사용되는 하이퍼파라미터, t0은 초기학습률, t1은 학습률의 감소 정도

def learning_schedule(t): return t0 / (t + t1) :  초기에는 학습률이 상대적으로 크게 설정 후 점차적으로 줄어들게 하는 함수

theta = np.random.randn(2,1): 초기 가중치(theta)를 무작위로 설정

for epoch in range(n_epochs):: 주어진 에폭 수 만큼 반복

for i in range(m):: 전체 데이터 포인트의 개수(m)만큼 반복

random_index = np.random.randint(m): 무작위로 데이터 포인트를 선택하기 위해 랜덤한 인덱스를 생성

xi = X_b[random_index:random_index+1], yi = y[random_index:random_index+1]: 선택한 데이터 포인트를 xi와 yi로 나타냄

gradients = 2 * xi.T.dot(xi.dot(theta) - yi): 현재 가중치에서의 손실 함수의 기울기를 계산

eta = learning_schedule(epoch * m + i): 현재 에폭과 반복 횟수에 따라 학습률 결정

theta = theta - eta * gradients: 가중치를 학습률과 기울기에 따라 업데이트
