cnn 이후 이미지처리는 cnn으로 사용
음성인식,자연어 처리, 이미지검색 등 활용

- 시각피질
모든 뉴런이 반응 하는것이 아니라 특정 범위 안에 있는 뉴런들이 자극에 반응한는 것을 관찰 함
뉴런을 저수준부터 고수준까지 함쳐지면 전체를 볼수 있을 것이라 판단
이후 합성곱신경망(cnn)으로 발전 하게됨

- 합성곱 신경망
첫 번째 합성곱 층의 뉴런은 합성곱 층 뉴런의 수용장 안에 있는 픽셀에만 연결 됨
합성곱 층에서는 수용장에서 반응하는 뉴런들만 학습
스트라이드: 수용장과 다음 수용장 사이의 간격
풀리커넥트? -> 이미지를 분류할 수 없음, 층이 적어도 학습해야 하는 양(가중치)이 너무 많기 때문
때문에 cnn을 사용함

- 제로패딩
padding="SAME" : 0으로 감싸기, 
스트라이드를 많게 할 수록 작아짐??

- 필터 (합성곱 커널)
입력 뉴런에 사용될 가중치 역할
필터의 모양과 크기가 국부수용장의 모양.크기 결정
ex) filters[:,3,:,0],filters[3,:,:,1]수직과 수평 필터

- 풀링 층
가중치가 없음
계산량과 메모리 사용량을 줄임 - 파라미터 수를 획기적으로 줄여줌

최대 풀링 층: 2*2 커널 일 때 4개 중에 가장 큰 값만 상위 층으로 넘김
많은 정보를 잃어도 잘 작동함 -> 왜 그럴까? 실험을 해보니 그렇더라 정확한 원리 등은 모름

평균 풀링 층: 커널 구역내의 평균값을 가져옴
AvgPool2D
최대 풀링층보다 성능이 떨어짐 -> 최대값을 유지하는 것이 보다 강한 특성이 남기 때문에

전역 평균 풀링 층: 각 특성맵의 평균을 계산함, 출력층에서 유용함

깊이별 최대/평균 풀링 층: 각 특성맵에 대해 평균을 계산함

- CNN 구조
입력층 -> 함성곱층으로 만듬 -> 풀링으로 줄여 나감 -> 완전 연결 층 (output 끼리 분류)
네트워크를 통과하여 진행할수록 이미지가 작아지고 층은 깊어짐
커널 크기가 크면 잃는 정보량이 적지만 계산량이 훨씬 큼
커널 크기가 줄어들 수록 계산량이 줄어듬
커널의 크기가 작을 수록 일반적으로 성능이 좋음

DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]) 28*28 커널사이즈가 7인 64필터 생성
keras.layers.MaxPooling2D(pool_size=2), 최대 풀링층 사용
keras.layers.Flatten(), 마지막 연결 단계
keras.layers.Dropout(0.5), 과대적함을 피하기 위해 0.5 드랍아웃 시킴
이들을 연결 해놓은 것이 CNN의 기본 구조

시험문제 - cnn의 구조

- leNet-5
손글씨, 숫자 인식에 사용
입력 벡터와 가중치 벡터 사이 유클리드 거리 출력
처음으로 합성곱아 등장하게 됨

- AlexNet
2012년 이미지넷 대회에서 우승
에러율이 상당히 낮았음, 이 후로 CNN 변종들이 많이 등장하면서 에러율이 점점 낮아짐
구조는 LeNet-5와 비슷 하지만 크고 깊음
처음으로 합성곱층 끼리 쌓음

정규화: 데이터의 분포가 고르게 하기 위해
뉴런의 출력값을 보다 경쟁적으러 만듬

- GoogLeNet
2년 만에 7% 이하로 닞춤
인셉션 모듈이라는 서브 네트워크 사용 ->  AlexNet보다 10배나 적은 파라미터를 가짐
가중치 파라미터 수는 줄었지만 성능은 올라감
 인셉션 모듈

- VGGNet 구조
2014년 대회에서 GoogLeNet 다음 2등
많은 개수의 필터를 사용하지만 3x3 필터만 사용

- ResNet
2015년 대회 우승
에러율이 3.6% 이하
파라미터 수는 점점 적어지고 극도로 깊은 CNN을 사용
스킵 연결로 보다 수월한 학습이 가능함 - 계산 줄임

- Xception
GoogLeNet 과 ResNet 모델의 합성 버전
에러율이 3% 정도
처음으로 함성곱층을 분리하여 연속적으로 적용
보다 적은 수의 파라미터, 보다 적은 양의 메모리, 보다 적은 양의 계산 요구되지만, 성능은 더 좋음

- SENet
2017년 등장 에러율이 2.25%
인셉션 모듈과 잔차유닛에 SE block을 추가
입력된 특성맵을 대상으로 깊이별 패턴 특성 분석

구조
전역평균 풀링 -> 밀집층 -> 밀집층
뉴런수를 줄였다가 정상화 시키는 구조

- Mobile
다양한 디바이스에서도 사용 가능하게 하려함
메모리가 작은 디바이스에서도 딥러닝을 사용하고자 함

MobileNet:2017년 구글에서 개발, 경량화 시키기 시작함

채널3개 커널3개 채널과 커널끼리 서로 모두 곱해져야 함
채널을 다 곱해서 하기에는 계산량이 많아 힘듬
채널을 하나로 하고 depth를 분류하여 계산
ex) 3*3*3*3 or 3*3*3*1 + 3*3*1*3 ...)

경량화 하고 파라미터는 줄이고 성능을 높이게 연구

- AutoML
사이즈가 작은 모델들을 보면 AutoML구조가 대부분임
=========================================================================
Q1: 풀링 층에서 원본 이미지 전체가 아닌 일부분을 가져와 학습해도 문제가 없는 이유?
-> 풀링 층에서 계산량을 감소시키기 위해 이미지의 공간 차원을 줄인다.
계산량을 감소 시킴으로써 학습 속도를 향상 시킬 수 있다.
원본 이미지의 전체를 학습 하지 않아도 이미지에서 중요한 특징을 추출하여 사용해 작은 노이즈나 불필요한 세부 사항을 제거 함으로써 계산량도 줄어들기 때문에 문제 없이 작동할 것이다.
   
Q2: 최대 풀링과 평균 풀링의 비교?
-> 최대 풀링은 풀링 커널 구역 내의 최댓값을 활용하여 특성 맵의 차원을 축소하는 기법이고,
평균 풀링은 풀링 커널 구역 내의 평균값 활용하여 특성 맵의 차원을 축소하는 기법이다.
추가로 최대 풀링은 가장 강한 특성을 이용하기에 뚜렷한 특징을 추출하고자 할 때 유용하고,
평균 풀링은 모든 값을 평균화하여 전반적인 특성을 이용하기에 세밀한 특징을 유지하는데 유용하다.

Q3: 최대 풀링층, 평균 풀링층, 전역 평균 풀링층 외에도 CNN에서 사용되는 풀링 기법?
-> 최소 풀링: 최소값을 추출하여 특성 맵의 차원을 축소하는 방식
중간값 풀링: 중간값을 추출하여 특성 맵의 차원을 축소하는 방식
