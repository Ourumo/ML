=========== 요점 정리 부분 ===========
경사 하강법 - 학습률 작으면 오래걸림, 학습률 크면 발산
  문제점: 최솟값으로 수렴하기 어려움

에폭 : 전체 학습 데이터셋을 모델이 한 번 학습하는 것
eta : 예상 완료 시간

배치 경사 하강법 - 데이터가 많으면 오래걸림

확률적 경사 하강법 - 속도가 빠르지만 불안정함

미니 배치 경사 하강법 - BGD와 SGD의 절충안

다항 회귀 - 각 특성 제곱하여 새로운 특성 추가
  degree 값 변화 -> 방정식의 차수 변화

과제 - SVM이 현재 활용되고 있는 분야에 대한 구체적인 사례

과소적합과 과대적합

릿지 회귀 - 알파 값이 커질수록 가중치 역할이 줄어듦(알파 값 = 규제) / 분산과 편향 관계, 가중치 값을 0에 가깝게 만듦

라쏘 회귀 - (릿지와 달리) 가중치 값을 0으로 만듦

엘라스틱넷 - 릿지와 라쏘의 절충안, 혼합 정도는 감마 값으로 조절(감마 값이 0이면 릿지, 1이면 라쏘 회귀와 같음)

조기종료 - 검증 에러가 베스트 값 즉, 최솟값에 도달하면 훈련 종료

로지스틱 회귀 - 선형 회귀와 달리 결과를 바로 출력하지 않고 로지스틱(시그모이드)을 출력 / 0과 1사이의 S자 형태 / 이진 분류

소프트맥스 회귀 - 로지스틱 회귀 모델을 다중 클래스 분류를 지원하도록 일반화 / 다항적 값 / argmax / 비용 함수로 크로스 엔트로피 사용(두 확률 분포의 차이를 구하기 위해)

=========== 문제 정리 부분 ===========
1. LinearRegression 역할,기능?
-> 리니어리그레이션은 사이킷런에서 사용하는 함수로 선형 회귀 모델의 구현을 위해 필요한 함수이다.
  데이터들의 표준을 보여줄 수 있게 하는 것이다.

2. 비용함수의 최솟값을 찾기위해 최적화된 스텝의 크기?
-> 처음에는 학습 스텝의 크기를 크게 잡았다가 학습이 진행됨에 따라 점차 스텝의 크기를 줄여나간다.

3. 경사 하강법에서 하이퍼 파라미터를 조절할 수 있는 구체적인 방법?
-> 학습을 여러번 반복을 하면서 비용함수의 크기가 이전보다 커진다면 이전 학습과의 중간값을 찾아 다시 학습한다.
  처음 학습을 진행 -> 비용합수의 값 -> 다음 학습 크게 -> 비용함수 값 감소
  처음 임의의 세타 값을 지정해준 뒤 학습을 반복하면서 비용을 줄여간다. 만약 비용이 증가한다면 현재 세타 값과 이전 세타 값의 중간값을 찾아서 다시 학습을 진행한다.
  임의의 세타값을 넣어 비용1을 찾고 다음 학습에서 찾은 비용2와 비교해 비용2의 크기가 작다면 세타값의 증가폭을 줄여나가며 하이퍼파라미터를 조절한다

4. 확률적 경사 하강법 코딩 의미? (코딩을 어떻게 짰는지)
-> 에폭을 50으로 설정, 적절한 학습 스케줄 파라미터 지정, 가우시안 표준 정규 분포를 따르는 난수를 생성...
  모델의 파라미터를 초기화하고 에폭을 여러 번 반복한다.
  에폭에서 임의의 데이터 포인트를 선택 후 선택한 데이터 포인트를 통해 비용 함수의 그래디언트를 계산한다. 
  eta(예상 완료 시간, 학습률)을 learning_schedule() 함수에서 계산한다.
  모델 파라미터 theta를 theta에서 학습률과 그래디언트의 곱을 빼는 방식으로 업데이트 한다.
  theta_path_sgd 리스트에 현재의 theta 값을 추가합니다.
  그래프에 표시합니다.
  
5. 데이터 셋 - 1000개, 에폭 - 100번, 미니배치 - 50개일때, 가중치 업데이트는 몇 번까지 가능?
-> 2000번, 미니배치 50개를 20번하면 데이터셋에 에폭 1번, 이를 100번하면 20 x 100 = 2000
  <추가 예시: 2500의 dataset을 크기가 100인 dataset 25개로 나누어 학습을 진행할 때, batch size가 100인 batch 25개가 생성되어 1 epoch당 25번의 iteration이 생깁니다.>

6. 다항 회귀에서 degree 값 변화, 과소적합되거나 과대적합되면?
-> 우선 다항 회귀에서 degree의 값이 변화하면 방정식의 차수가 증가하기 때문에 비선형 데이터를 학습하기 유용해진다.
  이를 통해 만들어지는 학습곡선은 과소적합일때는 데이터의 구조나 패턴 분석이 힘들다, 과대적합이면 데이터의 예측과 분류가 힘들다.

7. 릿지모델의 알파값을 변경하면?
-> 릿지 모델에서 알파값이 커지면 커질수록 가중치의 역할이 줄어들어 과소적합이 될 것이다.
  그렇다고 알파값이 0이 되버리면 과대적합이 될 수 있다.
