기계학습에서의 학습 - 데이터, 가중치, 비용함수

대중의 지혜: 한 사람의 전문가보다 여러 명의 모은 답이 낫다.
가장 좋은 값 vs 평균

투표 기반 분류기 - 여러가지 분류기를 이용

직접 투표 - 다수결로 결정
간접 투표 - 평균값으로 결정, 모든 분류기가 예측을 진행해야함

큰 수의 법칙 - 반복하는 횟수가 많으면 일정한 수준에 수렴함
 예)동전 던지기

voting=‘hard’ or ‘soft’ - 직접 투표, 간접 투표 

voting=‘hard’가 기본 값 - 직접 투표 방식이 기본

간접 투표는 모든 분류기에 예측값이 존재해야 함
(probability=True) 추가

배깅과 페이스팅 - 같은 알고리즘에서 훈련 값을 다르게 학습

배깅 - 샘플링 방식 중복 허용
페이스팅 - 샘플링 방식 중복 비허용

분류 모델 - 직접 투표 방식, 최빈값
회귀 모델 - 간접 투표 방식, 평균값

개별 예측기
편향이 증가함 -> 과대 적합의 위험성 감소
분산은 줄어듦 -> 과소 적합의 위험성 증가

(bootstrap=True)배깅과 (bootstrap=False)페이스팅

oob 샘플
배깅에서 선택되지 않은 훈련 샘플
개별 예측기 성능 평가 가능
(oob_score=True) oob 평가를 자동으로 실행
페이스팅 사용불가 -> 배깅과 달리 중복 비허용

BaggingClassifier - 특성에 대한 샘플링
max_features, bootstrap_features - 매개변수
랜덤 패치, 랜덤 서브스페이스 기법 - 매개변수로 특성을 조절함

랜덤 포레스트 - 배깅,페이스팅 방법으로 결정트리 앙상블 최적화
전체 특성 중 최선의 특성보다는 무작위성 -> 분산을 낮춤
무작위성을 위해서 데이터가 많아야 함

RandomForestClassifier vs BaggingClassifier DecisionTree
거의 동일

엑스트라 트리
특성과 특성 임계값 - 무작위 선택

특성 중요도
불순도 감소 -> 중요도 증가

부스팅- 성능이 약한 학습기 여러 개 연결하여 강한 성능의 학습기
순차적 진행 -> 병렬적으로 진행하는 배깅과 페이스팅에 비해 확장성이 떨어짐
에이다부스트, 그레이디언트 부스팅

에이다부스트 - 더 나은 예측기 생성을 위해 가중치를 조정하여 새로운 예측기를 추가하는 앙상블 기법
가중치를 업데이트하여 순차적으로 학습
learning_rate - 학습률
에이다부스트 알고리즘

그레이디언트 부스팅 - 이전 학습기에 의한 오차를 보정하도록 새로운 예측기 학습
이전 예측기가 만든 잔여 오차에 대해 새로운 예측기 학습
경사 하강법 사용
분류 모델과 회귀 모델이 RandomForest와 비슷한 하이퍼파라미터를 가짐
learning_rate 매개변수가 각 트리의 기여 정도를 조절
최적의 결정트리 수 - 조기종료 기법

스태킹 - 예측을 취합하는 대신 취합한 모델을 훈련
사이킷런에서 지원 안함
=================================================================================================================
Q1: 대중의 지혜 방법이 옳은 방법인가?
일반적으로 대중의 지혜 즉, 집단 지성으로 흔히 알려져있는 말이다.
초등학생 100명과 박사 1명이 있다고 예를 들어 생각해보자.
단순한 작업에 관련되어서는 초등학생 100명이, 복잡한 작업에 대해서는 박사 1명이 더 우수한 성능과 답을 낼 것이다.
그렇다면 우리가 실생활에서 어떠한 작업을 할 때, 초등학생 100명보다는 박사 1명 어느 것이 더 도움이 될까?
아마 박사 1명이 더 도움이 되는 경우가 많을 것이다.
결론적으로 기계학습의 한 부분으로서 대중의 지혜가 좋은 방법일 수도 있지만
우리의 실생활에서는 대중의 지혜보다는 한 사람의 전문가가 더 도움이 될 것이라고 생각한다.

Q2: 직접 투표와 간접 투표의 비교?
직접 투표는 예측값을 다수결 투표로 결정하고, 간접 투표는 평균값으로 예측값을 결정한다.
간접 투표가 직접 투표에 비해 근소하지만 더 높은 성능을 가지는데 그 이유는 확률에 의한 예측이 단순한 다수의 값보다 더 예측을 잘하기 때문인거라고 생각한다.
다만 간접 투표는 모든 분류기가 예측을 진행해야한다는 특징이 있다.

Q3: 배깅과 페이스팅의 비교?
배깅과 페이스팅은 동일한 예측기를 가지고 다른 훈련세트를 학습시킨다는 점은 같지만
배깅은 중복을 허용하고, 페이스팅은 중복을 비허용한다는 차이가 있다.
이로인해 배깅은 훈련세트를 학습함에 있어 중복으로 샘플링이 가능하기에 oob샘플이 존재할 수 있고 이를 통한 성능 평가도 가능하다.

Q4: 랜덤 포레스트와 엑스트라 트리의 비교?
랜덤 포레스트는 특성을 무작위로 선택하고, 특성 임계값을 최적값으로 선택하고 진행하지만
엑스트라 트리는 특성과 특성 임계값 모두 무작위로 선택한다는 점에서 차이가 있다.
추가로 랜덤 포레스트와 달리 엑스트라 트리는 bootstrap 샘플링을 사용하지 않는다.
이러한 차이로 랜덤 포레스트에 비해 엑스트라 트리가 속도가 빠르다.
또한 모든 데이터셋을 사용하기에 편향이 증가하고, 분산이 감소하는 경향을 보인다.

Q5: 에이다 부스트와 그레이디언트 부스팅의 비교?
우선 부스팅은 성능이 약한 학습기 여러 개를 연결하여 강한 학습기로 만드는 것을 뜻한다.
에이다 부스트는 이전 샘플의 가중치를 수정하여 더 나은 예측기를 만드는 기법이다.
그레이디언트 부스팅은 샘플의 가중치가 아닌 잔여 오차를 줄이는 방향으로 더 나은 예측기를 만드는 기법이고, 잔여오차를 줄이는 방안으로 경사하강법을 사용한다.
