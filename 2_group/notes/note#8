비지도 학습
지도학습과 비교하여 잠재력이 크다.

군집
비슷한 샘플을 클러스터로 모음
이상치 탐지 - 비정상 샘플 감지
분류와 군집 차이: 라벨링의 유무
군집을 사용하는 어플리케이션

K-평균
군집의 중심인 센트로이드를 찾아 샘플을 가장 가까운 군집에 할당
군집의 크기가 다른 경우 잘 작동 안함
처음에는 센트로이드 랜덤 선택 -> 샘플에 레이블 할당 -> 센트로이드 업데이트 반복
운 나쁜 센트로이드 초기화로 인해 문제가 생길 수 있음
일부 샘플만 사용

하드 군집 - 샘플에 가장 가까운 클러스트 선택
  [kmeans.transform(X_new)]
소프트 군집 - 클러스트마다 샘플에 점수 부여 (더 많이 사용됨)
  [np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, axis=2)]

K-평균 센트로이드 초기화 방법
이니셔: K-mean 모델 평가, score() - 음수, 높으면 성능이 낮다는 의미
elkan - 속도가 더 빠름
미니배치 - K-평균보다 빠르지만 이니셔는 일반적으로 좀 더 나쁨

최적의 클러스트 개수
클러스트 개수가 증가할 수록 관성이 작아짐 -> 좋은 성능 지표 아님
이니셔만으로는 모델에 대한 평가 불가함

실루엣 점수: 모든 샘플에 대한 실루엣 계수의 평균
[(b-a) / max(b,a)]
a = 동일한 클러스트에 있는 다른 샘플까지의 평균 거리
b = 가장 가까운 클러스트의 평균 거리

실루엣 계수: -1 ~ 1 사이의 값
1에 가까우면 자신의 클러스트 안에 포함되어 있고, 다른 클러스트와 멀리 있음
0에 가까우면 클러스트의 경계
-1에 가까우면 잘못된 클러스트에 할당

실루엣 다이어그램
칼 두께: 클러스트에 포함된 샘플의 개수
칼 길이: 클러스트에 포함된 샘플의 실루엣 계수 (길수록 좋음)
빨간 파선: 클러스트 계수에 해당하는 실루엣 점수
칼이 빨간 파선보다 길어야 함 (짧으면 다른 클러스트와 너무 가까움)
칼 두께가 비슷하면 크기가 비슷한 모델 = 좋은 모델

K-평균의 한계
알고리즘을 여러 번 실행해야함
클러스트의 개수를 미리 지정해야 함
클러스트의 크기나 밀집도가 다르거나 원형이 아닐 경우 잘 작동하지 않음 (긴 타원형 같은 경우)

군집을 사용한 전처리
미니 -> 전처리 -> grid

군집을 사용한 준지도 학습
레이블이 없는 샘플이 많고, 레이블이 있는 샘플이 적을 때 사용

레이블 전파
- 레이블을 동일한 클러스터에 있는 모든 샘플로 전파

DBSCAN
밀집된 연속적 지역을 클러스터로 정의
eps 내에 min_samples 이상의 샘플이 있으면 -> 핵심 샘플 = 밀집된 지역에 있는 샘플
샘플의 eps 주변에 있으면 이웃
핵심샘플도 아니고 이웃도 아닌 샘플은 이상치로 판단
두 개의 하이퍼파라미터 (eps, min_samples)
핵심샘플과 군집
이상치

DBSCAN 장점
매우 간단하면서 매우 강력한 알고리즘
군집의 모양과 개수에 상관없음
이상치에 안정적임
군집 간의 밀집도가 크게 다르면 모든 군집 파악 불가능

계산복잡도
시간복잡도: 약 o(m log m)
 m은 샘플 수
샘플 개수에 대해 거의 선형적으로 증가
공간복잡도

가우시안 혼합
파라미터가 알려지지 않은 여러 개의 혼합된 가우시안 분포로 생성됐다고 가정하는 확률 모델
가우시안 분포 = 정규분포: 종 모양의 확률밀도함수를 갖는 확률분포

EM 알고리즘
- 평균, 표준편차, 가중치
- 분포를 랜덤하게 만들고 라벨링함
랜덤하게 분포 제한 -> (likelihood 비교로 라벨링 -> 각 그룹별 모수 측정 -> 추정된 모수 이용한 각 그룹별 분포 도시) 반복
E-Step: Hidden variable의 reposibility를 계산하는 단계
M-Step: 추정값 세타를 업데이트 하는 단계

GMM 활용
GMM 훈련과 EM으로 파라미터 추정
GaussianMixture로 모델 사용

GMM 모델 규제
특성 수가 크거나 군집 수가 많거나 샘플이 적은 경우 최적 모델 학습 어려움
공분산의 규제를 가하여 학습을 도울 수 있음(full(기본값), spherical, diag, tied)

가우시안 혼합을 사용한 이상치 탐지
이상치 탐지: 보통과 많이 다른 샘플을 감지하는 작업
밀도가 임계값보다 낮은 지역에 있는 샘플을 이상치로 간주
정밀도/재현율 트레이드 오프
거짓 양성이 너무 많으면 임곗값을 낮추고, 거짓 음성이 너무 많으면 임곗값을 높임

GMM 클러스트 개수 선택하기
K-평균에서 사용했던 관성, 실루엣 점수 사용 불가
이론적 정보 기준인 BIC, AIC 사용 - 학습할 파라미터가 많은 모델에게 벌칙을 가하고 데이터에 잘 학습하는 모델에게 보상을 가함

베이즈 가우시안 혼합 모델
최적의 군집 수를 자동으로 찾아줌
최적의 군집 수보다 큰 수를 n_components에 전달
자동으로 불필요한 군집 제거
장점 - 타원형 클러스터에 잘 작동
단점 - 다른 모양을 가진 데이터셋에서는 성능이 좋지 않음

베이즈 정리(베이즈 확률)
사전에 가지고있는 지식에 대한 정보에 따라 사후 확률이 변함
데이터가 들어옴에 따라 사전 지식이 변화함

사전 믿음
군집 수가 어느 정도일까를 나타내는 지수
weight_concentration_prior 하이퍼파라미터
========================================================================
Q1: K-평균 알고리즘에서 사용자가 클러스트의 값을 지정하지 않고, 데이터에서 직접 찾아서 지정하는 방법이 있는가?
-> 우선 가장 이상적인 클러스트의 값을 찾아줘야 한다.
그 방법으로 생각한 것은 실루엣을 이용하는 방법이다.
실루엣을 이용하여 일정 범위에 속하는 클러스트 값들의 실루엣 점수를 매겨준다.
나온 실루엣 점수를 통해 이상적(일정 점수 이상)이라고 판단되는 클러스트의 값을 찾아준다.
이렇게 실루엣을 이용하면 사용자가 클러스트의 값을 지정하지 않아도 괜찮을 것이라고 생각한다.

Q2: K-평균 알고리즘과 DBSCAN 중 어느 것이 더 나은 것은? 두 알고리즘을 함께 사용할 수 있는가?
-> 
