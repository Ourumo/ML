#인공 신경망
뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델

#인공신경망 역사
1969년 마빈 민스키 - 다층 퍼셉트론을 학습시킬 방법이 없음
1974년 오류 역전파 알고리즘으로 다층 퍼셉트론을 학습시키는 방법 발표
오류 역전파 알고리즘의 그레이언트 소실 문제로 2000년대 중반까지 침체기
연결주의
- XOR문제
- 통계기술에 집중
- 전문가 시스템(통계기반)
오류 역전파 알고리즘
- 다중 퍼셉트론 학습
알렉스넷

#인공신경망의 부흥
컴퓨터 성능 개선, 대량의 데이터, 알고리즘 개선

생물학적 뉴런 -> 인공 뉴런 -> 인공 신경망

#생물학적 뉴런
뉴런 한개는 단순하지만 여러개가 연결되어 네트워크를 형성하고 있음
활성화 함수를 지나 결과를 도출

#인공 뉴런
생물학적 뉴런에서 착안한 매우 단순한 신경망 모델
이진 입력과 이진 출력
논리 연산을 수행하는 인공 신경망

#퍼셉트론
입력이 가중치와 연결
입력값과 가중치를 곱한 값들의 합에 계단함수 적용
임계치를 넘으면 활성화 함수

#TLU와 선형 이진분류
모든 입력값의 선형 조합을 계산한 후 임계값을 기준으로 양성/음성
입력층 - 출력층
로지스틱 회귀, 선형 SVM 분류기와 비슷
TLU(하나 이상의 은닉층) 각각은 모든 입력과 연결
최적의 가중치 wi 찾기

#퍼셉트론 학습 알고리즘
경사하강법과 식이 비슷함 학습률 출력값 입력값
오차가 감소되도록 가중치 조절
복잡한 패턴은 학습 못함, 선형적으로 구분될 수 있는 모델은 학습이 가능함

#퍼셉트론과 선형성
출력 뉴런의 결정경계가 선형
복잡한 패턴 학습 못함.
단순한 경우만 해결 가능
퍼셉트론 수렴 이론: 선형적으로 구분될 수 있는 모델은 언제나 학습 가능

#다중 퍼셉트론
입력층 - 은닉층 - 출력층
층에 속한 뉴런들 마다 모든 가중치와 연결

인공신경망 설명(https://wikidocs.net/37406)

#역전파
출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트
체인 룰에 따라 (미분의 연쇄 법칙) 가중치의 값 계산
역으로 미분해가면서 역전파의 가중치들을 찾아감
w5,w6,w7,w8 가중치의 값 계산
오차 토탈에서 미분해 나감
w1,w2,w3,w4 를 한번더 체인룰에 따라 계산해서 가중치 값 업데이트
인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복

#심층 신경망(DNN)
여러 개의 은닉층을 쌓아올린 인공 신경망
역전파와 순전파

#MLP
랜덤하게 설정
활성화 함수: 계단함수 대신 다른 함수 사용
- 로지스틱(시그모이드)
- 하이퍼볼릭 탄젠트 함수(쌍곡 탄젠트 함수)
- ReLU 함수

#활성화 함수 대체
시그모이드 -> ReLU
- max 값이 정해져 있지 않음
- 선형성을 벗어나기 위해
- 비선형 활성화 함수로 매우 강력한 모델 학습 가능
시그모이드를 계속 사용해가면 그레디언트 소실 문제가 발생함
ReLU 함수를 사용해서 기울기 소실문제 해결

#회귀를 위한 다층 퍼셉트론
MLP 의 전형적인 구조 (그래프)
활성화 함수는 보통 ReLU 함수를 사용
손실함수는 평균제곱오차 방법을 활용
다층 퍼셉트론으로도 회귀가 가능함

#분류를 위한 다층 퍼셉트론
이진분류, 다중레이블 이진 분류
다층 퍼셉트론으로도 분류가 가능함

#케라스로 다층 퍼셉트론 구현
케라스: 모든 종류의 신경망을 쉽게 만들어 주는 최상위 딥러닝 API
tensorflow.keras - 텐서플로만 지원하는 kreas 백엔드

#케라스 이미지 분류
tensorflow, keras 를 import 시키기
훈련세트와 테스트세트 나누기
훈련세트를 더 나누어 검증세트를 생성
라벨링 등 데이터 상태 보기
model.add(keras.layers.Flatten(input_shape=[28, 28])) 입력층
model.add(keras.layers.Dense(300, activation="relu")) 히든레이어 300개 렐루
model.add(keras.layers.Dense(100, activation="relu")) 은닉층 2개
model.add(keras.layers.Dense(10, activation="softmax")) 출력층 소프트맥스
신경모델 생성
층이 2개 인데 파라미터가 많이 나옴
확률적 경사하강법 사용 모델 컴파일

#모델 학습 곡선
시간이 지날수록 로스값이 점점 감소함

#케라스 회귀
잡음이 많기 떄문에 과대적합을 줄이기 위해 출력층에 활성화 함수를 사용하지 않는 하나의 뉴런만 사용
손실함수 - 편균제곱오차 사용

#함수형 API  사용
모든 레이블을 순차적으로 처리하는 것 대신 다양한 신경망 구축을 위해 사용 
ex) 와이드&딥 신경망 - 순차적으로 연결하지 않고 은닉층을 띄워 넘어 연결
가장 효율적인 신경망 구조를 찾기위해 시도

#동적 모델 생성
반복문, 조건문 등을 활용할 경우 명령형 프로그래밍 방식 요구
서브클래스API를 사용하여 동적모델 생성 가능
서브클래스를 사용할 때 
 - 모델구조가 숨겨져 있어 케라스가 분석하기 어려움
 - 모델 저장 및 복사 불가능
동적 모델을 만들수는 있지만 추천하지는 않음

#모델 저장,복원
케라스는 HDF5 포맷을 사용하여 모델 구조와 층의 모든 파라미터를 저장
옵티마이저 저장
모델 파라미터만 저장하고 복원이 가능함 - save_weights(), load_weights()

#콜백
대규모 데이터셋에서 훈련 시 훈련 도중 일정 간격으로 체크포인트를 저장
 - 중간에 학습이 멈추는 경우 체크포인트로 돌아갈수 있도록
callbacks=[checkpoint_cb] 체크포인트를 콜백에 넣어야함
사용자정의 콜백 - 에포크가 끝날 떄마다 검증손실과 훈련손실의 비율을 계산할 때 지정

#텐서보드
훈련하는 동안의 학습곡선
여러 실행 간의 학습곡선 비교
계산 그래프 시각화, 훈련통계 분석
3D에 사영된 복잡한 다차원 이미지 시각화 기능

#하이퍼파라미터 튜닝
학습률, 은닉층 개수, 활성화 함수, 가중치 초기화
하이퍼파라미터 조절에 따라 성능차이가 있음
자동으로 하이퍼파라미터를 튜닝하기 위해 그리드탐색, 랜덤탐색을 활용

사람의 개입을 최소화 시키기 위해 연구
(https://ettrends.etri.re.kr/ettrends/178/0905178004/34-4_32-42.pdf)
===================================================================================================
Q1: '인공 뉴런을 이용한 인공 신경망'을 '인간의 뉴런 신경망'과 유사한 수준으로 구축할 수 방법이 있는가?
-> 현재로서는 인간이 가지고 있는 신경망과 유사한 수준은 고사하고 간단한 정도로만 구축할 수 있다고 알고 있다.
물론 연구는 계속 진행되고 있다. 하지만 인간의 뉴런 신경망과 유사한 수준으로 구축하는 것은 불가능하다고 생각한다.

Q2-1: TLU(Threshold Logic Unit)와 로지스틱 회귀와 작동이 유사하다고 했는데 어떤 점이 유사한가?
-> TLU와 로지스틱 회귀 둘다 입력값(특성)에 가중치를 곱하고 합산한 값을 활성화 함수에 적용하여 이진 분류를 수행한다는 점이 유사하다.
그리고 TLU는 활성화 함수로 계단 함수를, 로지스틱 회귀는 활성화 함수로 시그모이드 함수를 사용한다.

Q2-2: 그렇다면 TLU와 선형 SVM 분류기는 어떤 점이 유사한가?
-> TLU와 선형 SVM 분류기 모두 선형적인 결정 경계를 갖는다.
하지만 선형 결정 경계를 TLU는 단일 뉴런으로, 선형 SVM 분류기는 서포트 벡터와의 마진을 최대화하는 방향으로 정한다.

Q3: MLP(다층 퍼셉트론)에서 은닉층의 개수가 많아질수록 훈련을 시키는 과정이 어려워지는데 이는 더 좋은결과를 가져오는가?
-> 층이 많을 수록, 훈련시키는 과정이 점점 어려워질수록 복잡한 문제의 학습 능력이 향상된다.
그렇지만 항상 층이 많다고 모델의 성능이 향상되는 것은 아니다. 층이 많아질수록 과적합의 가능성이 증가하고 모델이 학습하는데 있어 필요한 계산 비용이 증가한다.
또한 최적화의 어려움이 있을 수 있다. 만약 학습시키려는 문제가 작고 간단한 문제라면 층을 작게 두고 학습해도 충분히 좋은 결과를 얻을 수 있을 것이다.
적절한 층의 개수를 찾기 위해서는 다양한 층을 가진 여러 모델을 만들고 학습시켜 비교하는 교차 검증을 통해 모델의 일반화 성능을 평가할 수 있을 것이다.
